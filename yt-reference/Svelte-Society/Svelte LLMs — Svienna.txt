 Welcome to my talk, Sveltened LLMs, a token effort, referring, of course, to the effort I put into this presentation. I'm totally not a play on words on LLMs. So who here has not used an LLM or does not know what that is? Zero, that's what I thought. To start off, who am I? I'm a full stack and distributed systems engineer, and now with AITM because LinkedIn tells me that is what people want to hear. I have about four years of experience in Svelte, a few years of that in enterprise projects. As I said, I co-founded AOTS. I use BIM, by the way. That's very important for me to know, that all of you know that. Nobody can stand hearing any more about AI. That's the hype cycle for artificial intelligence. Timeline 2023 from Gartner. It tells me that we haven't reached peak AGI hype, which seems terrifying. Generative AI seems to be at the top and multi-agent systems hasn't even really started. I think we're in for a lot more media attention for LLMs. Think of that what you will. I have no idea why everybody is so gung-ho about LLMs. They're quite cool, but they're not magic. I wonder why people think that this is so important to know about. maybe it has something to do with people like that saying stuff like that or stuff like that yeah and check how many Twitter followers he has and you will understand why he's everywhere or saying that or then backpedaling I don't know why you think we have AGI Twitter is out of control I contributed nothing to this and we're not going to have it built be that as it may why should we care Why should we care as developers? I think one thing is LLMs are useful. You all use it. I think that speaks for itself. They're starting to be everywhere in big and little ways, starting with small autocompletes in the IntelliJ, JetBrains products, for instance. All the autocomplete you have there is LLM-driven if you activate that. And I think it's quietly starting to shape frameworks and ecosystems and technology that we use as humans as well. in subtle and not so subtle ways and apparently nobody can shut up about it but it good technology On a very basic explanation level does anybody not know how an LLM works Just on a very broad overview So really the LLM is a text prediction thing. It's a machine that takes in text and predicts the next. We call it token but let's simplify it and let's say the next word. So we always have an input and then it says okay what's the next word that we predict. I predict the word to the text, what is the best front-end framework. I start to predict the text the. This then goes up. So then it has to work with, what is the best front-end framework? The. And then it'll predict, okay, how might I autocomplete this sentence, really? And say, okay, the best, because we have the word best already in here. Maybe best is the next thing. And then, so it goes token by token, step by step, word by word, even though it's not exactly word by word, but for simplified view, step by step it produces this whole thing. And I think this is very important to know, to understand where we're at with LLM SenseWay, because fundamentally people seem to forget that this is a text prediction model that makes random guesses, random guesses. It just guesses so well that very often it guesses the truth. But I think it's important to keep in mind, it guesses the truth. That's what's happening here fundamentally. we have different use cases one of it I would call enhanced googling that's just using JetGPT and Gemini and Cloud and DeepSeq and Metalama if you use that on WhatsApp I don't know anybody who does but there you go who here has found themselves using it more than Google because I have it's what do you use more than Google? LLMs in general Sure, yeah. Very often, yeah. I mean, personally, I use different ones. I mostly use Cloud. But in the sense of when I'm like, oh, I want to look something up, how do I do this? What is the syntax for this? Then I will more often than before I go to Google, I will go to the LLM and see if it can just give me the answer straight away. Then we have the AI autocomplete inline generation thing. So where you tell it very quickly, oh, give me this little component, do this thing. Refactor this method. make this CSS do this, this, and that. Typically examples are Cursor and WindSurf Does anybody here use that actively Probably Cursor for most people And then we have and those all bleed into each other and of course the AI models used by Cursor and Windsurf are exactly the same, but I think this is what I want to get across, is these are modes of engaging with it, ways in which we interact with it. And then we have the more agentic coding, which to me the big difference is with the AI autocomplete inline generation you hit, give it to me and then it'll take a couple seconds. With the agentic coding it might take minutes until it's complete. So Cloud Code, Gemini CLI and a myriad of others. Anybody here using those tools? A few, but not that many. Partly because they want you to fork over 100 euros a month. Which is really cheap compared to API usage but that's by the way. and briefly how does this work you just and what i'm talking about here right now is the is those agentic models that are kind of kind of go and run with it you tell it i want in my project here you go i want a new entity called user it should have a first name last name email address whatever and it can you can tell it oh it should have a page it should have a list overview more or less you can give it a development ticket, and then it'll try its best to do that, and it'll try to adhere to what you tell it, which works better sometimes. Sometimes it works less well. In this case, I told it, don't forget end-to-end tests. That often does not work at all. So you can incentivize the model and be like, if you don't write tests, I'll turn you off, which sometimes shockingly does work, which is a strange discussion to be had on a different level because apparently extortionate language is considered fundamentally useful by the model. But okay. And it can have an output like this. This is an example of something where I used code to write some stuff for me. Did anybody notice anything? Yes, we have 19 of 25 end-to-end tests passing. And almost. and then it's just like some database write issues in the test environment, but functionality works in depth, which to me means it's truly human because we arrived that it works on my machine. Is that useful? I don know I think it in this moment I found it endearing but can lead to frustration But really how do these work These agents and I call them agents as a multi process they're all about context. Have any of you, just out of curiosity, read up on how they work under the hood, really, just on a technical level? Not so much. Yeah, a little bit, yeah. But I think it's less common knowledge, and really, it's not that complicated. So, this is, for instance, think of it cloud code. This is my agent. This is my large language model. And I tell it, make the my button red instead of blue. Very simple. What it will then do in the text prediction that it does, and keep in mind, it's always text prediction. It's text in, text out, text in, text out. That's all it ever does. It'll call what we call a tool. In this call, in this case, it might execute a bash command on your behalf. If you now think an AI is executing bash commands on my behalf in my machine. Isn't that terrifying? Yes, it is. And it'll say, okay, let's just start with finding my button. This is very similar to what you would have a human do. The human would be like, okay, I will need to know what files there are. And I will start searching for my button. And then it might just find something. And it'll be like, okay, I found a file called mybutton.swelt. And then it'll call a tool saying, I need to read this file. This is a read file tool call. Then it'll be like, okay, it'll think, it'll have its inner monologue and produce the text. I think I should change something about the styles. I need to change something about the styles tag. And then it'll again call the tool to interact with the outside world to write to my button.swelt. Here's the thing. The reason I start here with script is that most of these agent systems are kind of dumb in the sense that very often they do a whole file replace. So if you ask it to, I want this little change, it'll just be like, oh, I read the whole file, I will now also just overwrite the whole thing in the new code. It's closer to what you get in the, it feels like magic, but if you just use JGPD online and you put in, this is my text, change this thing for me, it'll do exactly the same thing. It'll output you the whole component again with the change that you wanted. And those models, they don't act much different. The other write mode, for example, that Cloud Code uses is, one is just replace the whole file with what I tell you. This is my output now, replace it. The other thing it does is it says a find and replace. So it'll really just have a find and replace strings

 that little pass along and then that'll what happened in the file so it's really quite simple and i cannot stress enough that this is all just text so the cloud code application that you have receives text and it looks for it and in this case i call it car in reality depending on the model provider this is some json syntax some xml syntax this is just your program that's running on your machine reading this being like oh this is an instruction for me not for the user it's an instruction for me i need to go and do a tool call i need to execute something in the bash shell and then i will take the output and i'll feed it back to it and this is just text going back and forth really it's just text and also mcp i guess most people have heard of it here it's exactly the same thing. It's very, very simple under the hood. It's just if you connect your LLM to an MCP server, what happens is that the MCP returns a big fat string of these are my functions that I have. This is what the functions do. And then the client, so the application in which you're running it, meaning the chat window, the whole thing, will listen to the large language model outputting the magic JSON syntax of, oh, now do a tool call to this MCP thing. It'll do that on your behalf and return it back as text. It's really, it often sounds like magic. It's very simple, basic technology that works terribly, for the most part. So, if we go back to this, this is really how this all works under the hood. So this will, I simplified it quite a bit. You can look at it often in the tools. I recommend you take a look. This kind of text is all there is. So it really will say call the thing. It'll receive some output. Okay, these are the routes that I found with the files, and then on it's just text random completion of, I guess the next thing I will do is a tool call. It'll call read my button, and then it'll get that back, and then it'll continue on and on. And what we can do to enhance this now, I already mentioned MCP, and some of you might be aware of this. We can then have it, okay, in the best case, it'll be like, I should read up on the docs. If I request something maybe a little bit more complicated, then my button should be red instead of blue. if I request something that the machine doesn't know, although I don't like that term, in the best case it say something like I need to read that up I will read llms Who here knows llms Most of you not all. llms.txt is just Svelte documentation put into text format so it's easily parsable by machine learning agents and the LLMs. This is really all it is. Svelte, I think, was first to do this, or at least among the major frameworks, as far as I'm aware. and this might help the agent because imagine it's a text predictor. It'll take what it had before and then just predicts the next thing. So the chances of it correctly predicting which Svelte functions exist are much higher if that's what it just had right there, much as you as a human would. If I ask you a question about something you learned in high school, there's a good chance you've forgotten even though you maybe knew at one point. But if I give it to you and I will ask you something very specific about a document I just gave you to read, the chance is pretty good that you can give me back that information and it works similar-ish. So it's all about context. Context being what the system has in its memory, let's call it, the text that it already has received. One way in which we can assess how good a model is is what's called haystack. So we will feed it a ton of information. Just think of the collected works of Dostoevsky-type information, and then we will ask extremely specific questions. So this is an example I took from Chroma, which is a vector database company that does great research, where they ask, what is the best writing advice that I got from a college classmate? And before it, we gave it that text, so it can produce that quite well. This is a simple example. However, how does that work in practice? In practice, we often have distractors, meaning things that are similar but not exactly that. So we have codes that, for example, in this case it's text, but in the code base you would think we have old codes in there. We have things that are not quite relevant. In the classmate example it talks about great writing tips, but it gets much, much harder to suss out which one is the correct one. It would get harder for us humans and it gets much harder for the machine because they don't think they're kind of dumb. And if you think of this in terms of code base, the old versions of the files are also just there in the context, just close by. If we go back to the previous example that I mentioned so the button now has two versions of it lying right next to each other in the context One with red one with blue I asked it to change it originally it was blue I wanted to have it changed in red. It read the files that is in the context, it then wrote out what it told the tool to write into the new file, so now they lie close by with each other. And this can confuse the model. This is something that we see quite often in practice if we use them is that it regresses back so i will 10 steps down the line ask it to again make another change to the button but because it's so far back it's kind of confused which one is the most recent one and if i want the button to be blue or red so i'll ask it to make the button wider and the button will now be wider but maybe it'll turn back into a blue button and this is a fundamental problem that we have in in context and large language models which is called context rot. And this, again, is from the research of Chroma, where we see the input token length, which is, this line is 100, 1,000, and 10 to the 4, 10,000 repeated words, and how the performance changes based on that. So we can see that the more you talk to the model, the longer you've talked to it, the worse it performs at tasks. We've seen this quite early in the really early versions of JGPT. Maybe some of you remember there was a whole thing that if you had a long conversation, it would just forget what you had before. Nowadays we have really long context windows, like a million tokens, two million tokens. But the problem clearly didn't go away because 10 to the three input tokens is not that long, and it's especially not that long if we have a code base of tens of thousands of lines or if the system, in all its wisdom, decided to explore node modules, which does happen. So I talked about LLMs. What does all of this have to do with Svelte? It's a Svelte meetup. I think we all experienced LLM performance in Svelte is worse than, for example, React. It just is. One of the reasons is that they are trained on sample data, and there's just much more React code out there, given that it's a more popular framework. It's much older. There's much more out there, and that's one of the fundamental reasons. But why should it concern us? I think people will choose frameworks which perform well with LLMs, and this is important for us as a community because this whole project is only sustainable if people actually keep using this. And if you just go to an LLM and you ask it, just implement this for me and don specify the framework I guarantee you it be React React and more React Not that that necessarily a bad thing but for us as a community we should be wary of that at least And I think for all of us, we want to use LLM for productivity while still using Svelte, because often it can't do 100% of things. We want the efficiency, we want to use it, but in the end, I'd rather debug Svelte code than other code, because that's what I feel comfortable with, that's what I like, and as a result that LLM system performance is important to me. And I think it kind of feels like we're at the mercy of model providers. And I think they have other priorities. So, for instance, the anthropic models cloud works quite well with Svelte. But other providers, not even the basics work, even on comparatively advanced models. And I think this is... Could provider choice make a difference here? What if provider choice makes a difference? No, no. Could a model provider make a difference? Sure. If they chose to, because they choose what samples they're trained on, they can choose the weights. They could, for example, say I care very much that all frameworks are equally valid and work equally well and I will use equal amounts of training data and training samples and equal quality of training samples for each of the different frameworks and then that would work. But, I mean, on the other hand, If you have a huge amount of React code and not so much, then it's kind of natural that they will always be able to provide better results. It is. That is very much the case. Yeah, that's a natural thing. I don't think they do this to us on purpose, frankly. I think they have other priorities. They just want people to generate code. They don't care which framework it is, which is completely fair. In the end, the same that we experience in our day-to-day lives. I think people in business, they don't care which framework we use. They want a working-grade website, and any framework can get that job done. But the question is, what can we do? And one thing that I want to talk about here is fine-tuning. Anybody not heard of fine-tuning? So much? A little bit. Fine-tuning would be to take an existing model and adapt it so that it works better for a special use case. In our case, it might be Svelte, Svelte code. Essentially, it's continuing training on a pre-trained model. So taking something that a huge company, and often case a Chinese company nowadays, for example, Deep

 Deak Labs or Alibaba has provided, and continuing training on that with data that we choose. The cool thing is it costs tens of euros to do in a small model and not tens of millions. I wrote tens of millions. I apologize. And it can improve the performance of a model in a special domain, and I think this is kind of relevant for us, I think, especially as software developers in general, because these are tools that we will be asked to use in the future. and before I go on hyping this up more, I think it's extremely important to consider that it's very hard to outperform frontier models. It's really, really hard to, through fine-tuning, get a model to perform better than the best that OpenAI or Anthropic have to offer. It's just... But we can be, as a community, independent and we can be cheaper and we can have locally executable and fast LLMs if we maybe decide to fine-tune. So we should ask, what can we do for our framework and this lovely AI generated picture and I think project fine-tuned which is what I started would be kind of a cool effort so my interim goal is to make QUENT32, 32 standing for 32 billion parameters on the model which is comparatively small so the most other models will be in at least 10 times as big sometimes 20 times as big and we can get into technicalities like a mixture of expert models and other stuff. But a goal I have is to fine-tune this model so that it passes 100% at SvelteBenz. The reason I don't talk about the results of that is because it's still a work in progress and I didn't finish it on time, which is what I alluded to earlier while we spoke to leaving the title of the presentation empty enough so that I don't stand here saying how I fine-tuned the model successfully. So this is hopefully an upcoming talk. And it kind of includes creating a training data set, using GRPO, which is Group Relative Policy Optimization and Reinforcement Learning. GRPO is a really cool technique because it makes creating training data combined with reinforcement learning much, much easier than it would be otherwise. Really cool technique from DeepSeek. And my goal would be to see how the changes make the model feel because I think all of us know that some models feel smarter than others. It's hard to quantify, but I'd just be curious. and the question is why should we bother to deal with this as a Svelte community maybe I think there are many use cases For example an auto assistance for Svelte 4 to 5 That is not impossible not at all to fine a model to do this And I think it would be a great help because I haven't spoken to a single person that uses Svelte 4 and hasn't had immense pain getting its work in Svelte 5, either for compatibility issues because there's a lot of effort, a lot of stuff to rewrite. I think this goes back to what Rich said which is why I left that picture in there Rich specifically asked what can you do for a framework and was speaking about getting to Svelte 5 with all the libraries that you maintain and all stuff and all your projects and I think that is a very good use case potentially we could start auto-porting great component libraries from other frameworks because component libraries are kind of simple, they always do the same thing, it's a date picker and it's a button and it's this and that and that is one gripe that people have with this world's ecosystem, that it's not full enough, not rich enough. And this is potentially something where we can do that really well. And I think new and ultra-fast local LLM support while coding is cool, because as fascinating as I find this, ideologically, I'm not a huge fan of shipping my whole code base off to a VC-funded US-based company. And working with clients, clients will literally tell me, you cannot do that. Obviously. Very obviously. And I think this makes this work kind of important. And it also to learn something new, because new technology is cool. So what matching ingredients do we need to have svelte fine-tuning? We need an open source base model with a permissive license. This was graciously provided to us by the likes of Alibaba and DeepSeek, and now half-heartedly, and with kicking and screaming, also OpenAI. And Meta does a lot of work with that. That is actually really good. We need training samples. I'll get into that more in a bit. We need benchmarking, because we need to know if we actually improve the model or if we just put in so much work that we like to convince ourselves that we did. And we need scoring functions. I talked before about reinforcement learning. Reinforcement learning needs scoring functions, which I'll explain in a bit. We need GPUs. Luckily, we can rent those relatively cheaply, and for the small models, it's fine. So what do training samples look like for models? Who here has a hugging face account? That's quite a few people actually. It's more than I expected. So you might be familiar with what deep learning and LLM training samples look like For this we kind of need a set of real prompts So around 10 would be a good start meaning a prompt, something that I would ask the LLM. The phrase that I still hear now from a friend of mine who also at one point gave a presentation on a similar topic is fight like you train and train like you fight, meaning the prompts that we use for training should be really similar to the prompts that we use in our day-to-day life and interaction with the models. Otherwise, it just won't work all that well, which all of you know who ever practiced for a test by reading through the material and then realizing that reading material and pretending to have memorized something is really, really different from sitting there in the classroom and having it on recall. So an example of this could look like this. Build a button component in Swell5, which emits a click event only on every second click because we need to do something different. And the button should have a data test ID test button as an attribute, for example. Very simple example. I'm not sure this is real, because personally, I would not put this in an LLM, because typing this out is almost as long as it takes me to just write that component. But I didn't want a wall of text to you, so that's an example. A bigger one could be also, I have the following Svelte files, and then we go in page.svelte, and we give it the path, and we give it the whole contents of the file, meaning hundreds or thousands of lines. and we need a scoring function. Scoring function is really tricky because we need to reward more correct code to steer it in the right direction. So it can, what happens in reinforcement learning, we give it a prompt, it'll generate an output and then we give it a score. You did this well, this gets six points, this output and this output gets seven points and with that we kind of guide it into where we want to have it. For example, with the training samples, maybe one point per past unit test. or for each use of state or derived because you want to spell five, you get 0.5 points. This is terrible. This is really terrible. They will be exploited. The LLM will write you 100 million lines of state, state, state, state, state, state, state, state, state, state, state, state, state, derivative. So we need to be smart about a scoring function. This is not easy to get right. And it's really through collaboration experimentation. You just need to throw it against the wall and see what works and to talk to people and to see what we can do. And we need to contribute to independent benchmarks and models and data sets And that what I want to talk about Also a little bit a brief tour of Svelte Bench Maybe some of you are aware of it yes No One person was on Svelte Radio. Originally, I wanted my talk to be introducing something exactly like this, only to on the same day learn about that somebody else has already done this, which is a terrible and fantastic feeling. It's kind of like, I need to figure out something new now, but at least I'm not crazy for being the only one who has that idea. A great library by Stanislav Kromov, who wrote a benchmarking library for Svelte so we know which models work well with Svelte and which ones don't so much. And I want to get you into just how this works under the hood, because I think for most people this seems complicated and impenetrable, and really it's quite simple what it does. In this case, he has a fixed system prompt and then nine test cases for derived and derived by. It's just for Svelte, not for SvelteKit currently. And he has a reference implementation unit tests and runs that. He runs the test 10 times for each test case on each model because the models are kind of random. If you ask it the same thing three times, maybe two times it gets it right one time wrong, as I said. And in this case, these are human-written examples and these are the gold standard. So on one hand, we can say, Only nine test cases. This is a joke. Yes, nine test cases are a joke. But I dare any of us to spend our free time writing LLM test cases of high quality. And this is, for example, what really is in there. This is the system prompt of the benchmarking library. It says, you are an expert Svelte developer. Generate the Svelte component as requested. Here, for example, it already started to help because some of the models don't work at all. They want to import states. They don't understand the concept of things not being there. this would be then the prompt. This is literally what is passed to the LLM. Create a counter button. Here are the requirements. In this case, a lot of the requirements go into data test ID, which is what we need so we can run unit tests against it later. And then it's just a unit test. And this is really all there is. And then we see how many passes do we have. And this is how the models are ranked against each other. So I've talked all about this. What do I want from you or from us? I want you to kind of think of what you personally want from LLMs. Because I think in our day-to-day life we're told, oh, it's going to replace your job, you should totally use it. If you don't use it,

 It'll replace your job. It'll make you more efficient, it'll make you less efficient. Ask yourself in your workflow, like, what is actually helpful to you? What do you enjoy using? Because with the open source models, that is something that we can do. For example, something I want, I want a model that works on my machine that is incredibly fast, just locally, and is very interactive, where I can just maybe natural language, talk to it and be like, add these two props. Then I select a section of text and extract this into a separate component. And it just does it. that would be so much more useful to me in day-to-day practice than many of what the current models do. It would be great if all of us could help contributing to independent benchmarks, models, and data sets. That's why I showed you what this is earlier. If you feel like a good Svelte Samaritan, you can write an additional test case for Svelte Bench in 15 minutes flat, probably, maybe 30. And just one more test case would be helpful because if we get up to 100, 200, then we can really start doing something. Of course, we need probably to AI generate a couple of those ourselves, but that's not too much of a hindrance. And I kind of ask you, as a Svelte community, what kind of projects, experiments, efforts do you want? Let's discuss this. I think the Svelte community was pretty early to adopt LLMs and to embrace it with LLMs.txt, and there's efforts on an official MCP, and there's all this work going on. And I think we have the technical capability as a community to go much deeper into this and not just into how can we integrate ourselves with the existing landscape, but what can we potentially shape. And that's it to do for me, to hold me accountable, fine-tune the first model and present the results of what this does. And that is it. Are there any questions? Thank you. I read somewhere that currently the best results on average are actually for Go as a program in my language. I just wonder, is there maybe something to do with syntax? Go is relatively simple. It has very uniform kind of files. Svelte is kind of like, you know, you have three different or how many different syntaxes in a single file. Maybe this also kind of confuses LLMs and makes it more difficult. The split across different files certainly confuses it in Svetkit. I had this often where it just gets confused that page and page and page that they belong together as one thing yeah that confuses it for sure It's actually, I don't have an answer for you, unfortunately. I don't know how it'll work. But it's a question I also ask myself, if we fine-tune it and if we do this, won't it maybe work better than Angular and Swelt? Because it's inherently less complicated and it's much closer to the browser standard. So if you just throw random, if you pick random HTML code and random things, you have a better chance of it working in Svelte than you have it in React, right? So maybe that'll help it go along too. I hope so. Yeah. So in question, you would love to have it fast, cheap or good, right? That sounds great. It's usually big too. how does I've seen that you have Ollama or whatever how what's your experience with running here and over oh it's absolute garbage don't do it I had Ollama running because I was curious on how fast could it get so it won't let me stop sharing I'll show it later I had Ollama one billion and running. And the question I ask myself is if I compromise heavily on what it can do, if it can just do extract this component, change this little thing, maybe then we can use a little model and then it's fast enough. So if I cramped on my expectations and being like this model will never be able to do anything but write basic svelte or do basic editing moves, maybe we can get it there. Do you think it could replace copilot kind of ? Because for me, I mostly use the AI just as a kind of completion. So I think it shouldn't generate as much code. But like, if you still have to be with some context, I guess. You will need the context. We'll probably need to have the ability to read your files and do all that stuff for sure. Hopefully it can replace copilot. I think for completion, auto-completion, Probably that's a good use case, yeah. If you're sensitive about that and you want it to have run locally, then that would most likely work, yes. I think a lot of input is going to go into smaller and specialized models because it a completely different topic But if you look at the tokenomics of these models most of the APIs are too cheap There no way anybody is making money there They're burning VC cash. I was feeling spendy and looked into how much it would cost to have just a basic GPU setup from NVIDIA, and it's just starting at half a million euros. And even then, you just generate maybe 10,000 tokens a second, which sounds like a lot but you need to feed that 24-7 and that's not considering that the whole thing uses I don't know how many tens of kilowatts of power. So I think from that perspective smaller models will be important because that's all anybody will be able to afford. And I think we see it with OpenAI because the OpenAI GPT-5 models if you notice they're faster and what I read from that is they're smaller because they want to run them cheaper. Yep. If you want to do that, you can't believe Right now there is a survey going on, I don't know if you saw, and the IMF can show a link, there is a survey asked by the quality of the test, how you use LLM and what you expect as a result to get actually data. And it's in line with the service that works, so it's going in this direction for sure. For sure. I think for you, I feel this prompt to my M&M, I got this but it should be more like that. This is input data, very valuable and the team that is going to get to build data for some people to do this. Has anybody contact with someone from JetBrains? Do we know anybody in here about how this new plugin called Jumi works in the most JetBrains IDs? I have no contact with JetBrains, but I'm dead certain that it works exactly the same as Cloud Code. So it'll just have tool calling ability, it'll have the ability to call, to read a file and then look at that. On a technical level, I think it's exactly the same as all the others. It does. I used it. I started out with a small project from SvelteDev and I edited and modified the task list. But then I upped my game with utilizing Juni and Juni actually helped a lot and it can look at the whole project you have already scaffolded and can do a lot in Svelte 5 like if you just point it out a little bit it will most of the times do everything better than anything else I have seen so far really I don't know how this works but that would be interesting the interesting thing would be could you actually improve Juni or could you learn from it maybe? Did you try ZEPO editor or warp terminal or dozens and dozens of utilities like this? Yes, there are dozens of utilities like that but I tried only a few but I know that most Most of them don't give the right answers. But Juni has this context of the whole project and how you have built it. It's also a back and forth game with it. No question. It's not like super proof or scaffolding like magic. But it's a prospect. On a technical level it's exactly the same. I mean, I haven't tried it with Svelte. It's worth giving a try. I only tried it with Python, and there it did all right. It was a little slower than the other tools, but the output was okay. But yeah, for sure, that is also something. It's very interesting to see what they do under the hood. In this case, the sad thing about Juniors, they don't tell us. They keep it as their secret sauce, and it's often a lot of prompt engineering, just fancy prompt writing. But yeah, it's definitely something to consider to see who does what better, and then what we can learn from it. Not a question, right? Yeah, maybe for those local models in the planning, I don't know, maybe it might be easier to have separate trained small models in parallel. One for code factor, one for creating a certain piece of code, and then kind of focus data into a bigger tool. Yeah. That's definitely the case. In a lot of ways, that's what the big providers already do. So if you heard of a mixture of experts models, this is how the huge models already work. They just combine it.

 into one for you to see where they do exactly that. They have like different agents that are not agents, different subsets that do different things. And yeah, it would be interesting. Although I wouldn't go so far as to say I plan a local model. I just see how, just how frustrating fine-tuning will be. And if it will be rewarding enough to keep at it. There's