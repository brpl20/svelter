 Welcome to Svelte Radio. Hello, everyone. Welcome to another episode of Svelte Radio. Today we bring you yet another guest. I'm joined, but before that, I'm going to say hello to my beautiful co-host, Anthony. Hello. Hello. Hi. Brittany isn't here today, so we only have one extra person, other than me, of course. But we have a very exciting guest today, Stanislav. Hello. Welcome. Hello. Thanks. Nice being here. So Stanislav, you have been breaking the internet with your Sveltebench project. No, I'm kidding. So you're a Svelte ambassador. You have been quite heavily involved in the Sveltebench stuff. So benchmarking Svelte and LLMs. but before we get into that, let's hear from you. Who are you and how did you get into Svelte? Yeah, we were talking about this just before the show and we were trying to determine when did I actually start using Svelte and get to know Svelte and I think it was somewhere around 2022 and I work in editorial products so we do like newspaper stuff, we do visualizations, we do articles and I think we had heard about this amazing new framework by Rich Harris when he worked at New York Times, I believe. And we were actually already experimenting with it. And I just remember it was pre-SvelteKit 1.0. We were using the 0.something version at work. And we were also using a lot of vanilla Svelte in our visualizations. So the kind of visualizations you're talking about or stuff like election graphing and stuff, that kind of visualization that you would do on a newspaper website, right? Yeah the kind of things that Rich originally made Svelte for So kind of like really really performant and really really easy to write visualization when you have to do things quickly And that was a long time ago. Now Svelte and SvelteKit is much, much bigger than that. You can do basically anything. You said you got into Svelte around the SvelteKit beta. Was this before they switched from the old routing system to the new one? I barely remember the old system actually no I still remember it wasn't it like underscore underscore page or something we didn't have the plus page yet I think it actually was just the route name dot svelte and index or dot html even before that that's svelte 2 right I mean it was Sapper really I suppose but yeah because it's all used Sapper so I know all about Sapper which is not much fun it's crazy to hear that you're still using Zapper Puru is migrating as a way I would say almost single handedly which is brilliant the underscore was for certain files like was it like layouts or something I can't remember or was it counting a layout it's been a while since I've done that stuff but yeah it had a weird syntax yeah but it was still great but the new system is better for sure I quite like it Yeah, I just remember that Zapper was already on the way out, that the SvelteKit 0.something was in the making. Yeah, SvelteKit 0.something was the middle of Zapper, extracted, copy-pasted into a new project, and then new stuff wrapped around it with Snowpack. Wow, it's so cool to hear the OG peeps remembering all that stuff. I came in like unbothered, you know, like completely like fresh to the scene with Svelte Kid 1.0. Yeah, but look at what you've got with it. I mean, it's amazing to see, you know, to see like upskill and stuff is crazy. I would say that you're right about being OG. I think that when we do this podcast, we meet a lot of people who've been in it for various amounts of time. But the thing that amazes me is everyone goes, oh, do you remember old school days of Svelte? I'm like, yeah. And they're like, you know, when you just had like double braces. I'm like, no, it was before that it was way worse. you know there was i felt felt one was was very raw and it was great like that's why i'm still here now but it was it was very much like there wasn much to it you know yeah i remember i remember penguin saying or was it you anthony like there were like 40 people in the Discord at some point Might have been Well I think I joined the Discord when it had just been moved from whatever they were using before Discord So I don't remember what it was before. I didn't see what it was before. There was something before? Yeah, there was. Probably that open source thing. Yeah. Yeah, Gitter or something like that. Yes, yes, Gitter is the one. How many do people have on the Discord today? Does anyone know? I think it's like 80,000. Yes, some crazy amount. I mean, it's the amount of active people as well, which is amazing at any one time. It's super fun to see how big it's been growing over the years. All right, so you got into Svelte, and you've been using Svelte for a while then, Because it feels like you not only used it at work, but you also got kind of interested in using it for side projects as well, right? You built some capacitor apps. We mentioned it before the show started. But then now you've most recently, I guess, worked on the Sveltebench and the MCP. So let's talk a bit about the Sveltebench. What is it? well Sveltebench is a way to sort of grade an LLM on how well it knows about Svelte 5 specifically because when we had Svelte 4 when did ChatGPT come out? it was like in 2022 or 2023 it was somewhere around there when did the world change? yeah it feels like it's been around forever but it really hasn't right but somehow because I guess Svelte was kind of HTML-like. I feel like Svelte 4 was pretty okay for LMs to write. But then when Svelte 5 came and I kind of started testing it out, it was so rough to do AI-assisted coding with it because I was just hitting my wall against a brick, hitting my head against a brick wall for like two weeks. And then I was just randomly on the Svelte.dev and I loaded it up in Network Inspector and I found this file called content.json which is what powers the search in the top menu and I realized that all of the SvelteKit docs are in that file So I tried just drag and dropping that file into my AI and it got a lot better at doing Svelte 5. And that's kind of when the first realization I had that like, okay, we need to maybe teach the LLM stuff in order for it to work really, really well with Svelte 5 at that point. Yeah, it's interesting to see. So I guess Svelte hasn't really changed a lot since Svelte 3 came out in 2019, right? So, well, it has now with Svelte 5, but like from Svelte 3 to Svelte 5, that was probably like five years of no changes, other than like small changes to how transitions worked, if I remember correctly, and some stuff like that. so I guess the LLMs also had a long time to train on actually Svelte 4 or Svelte 3 code so it probably has a lot of the old stuff in its I don't know if you would call it memory? Layers? I don't know what I'm saying Learnings, yeah so it's understandable that the LLMs have a hard time writing Svelte 5 because there isn't a lot of Svelte 5 out there yet compared to Svelte 3 and Svelte 4. But what do you think has surprised you? Actually, before that, how did you build Svelte Bench? Did you use a lot of LLM to scaffold the user interface? That's meta. That's very meta. Yeah, it is always meta to write AI tools using AI. But yeah, I did. Actually, when I sat down to do Sveltebench the first time, I needed some way to basically ask the LLM to write a Svelte component and then test it. And then I was like, okay, maybe we can use something like Vtest for this. And I kind of like wrote my prompt and I pressed enter and it was a total disaster because it just, the LLM I was using, I was using Cloud, I think at that point, I just couldn't figure out how to connect, how to dynamically get the code from an LLM and kind of put it into Vtest. That was just not something that the LLM, I think, had ever maybe seen before or didn't have enough examples. So that was a great point about the limitations

 LLMs that like they can do things that they already know that they've seen before but when you try to get them to paste together new stuff and like make something that hasn't really been done it's much tougher so I actually ended up like taking a weekend and like spending some real engineering time in quotes to actually like connect to the like hooking up vTest with like an AI like SDK that could like dynamically generate a component and actually and then vTest could test it and And then once I had that working, then the AI was just saving me so much time because the base was already done. And now I needed to scale up. I needed to write new tests for different runes and different Svelte 5 functions. And that it did perfectly. And it made a beautiful dashboard to show off the data. So that was amazing. I still saved a lot of time. But also, over time, I've learned when you should stop trying to get the LLM to do something and just do it yourself. because it's just not going to be able to figure some stuff out. Yeah, I've recently, when it comes to working with LLMs as a tool for when you're building stuff, I've learned that writing PRD, as it's called, I think that's a product, PRD, what does it mean? You see, I know the abbreviation, but I don't know the actual. So what you're saying is you have a, you have a shallow knowledge of these things. Oh, a hundred percent. I was running your life and you just, yes, yes. Product requirements document. It's basically like what you get from a product manager or product, uh, uh, whatever. Uh, and it just outlines the functionality that you want and then how it should be implemented in. So it splits it, it splits up the work into different phases. and then you start with one phase. And then obviously before you start working on the actual code, you iterate a couple of times on the PRD document, like maybe split up some functionality even more. I feel like that helps a lot. You don't want to give the AI a prompt like, build me a tool for communicating with people. And just have it generate the whole application. You kind of need to be very specific with how to... how to work with it, I guess. And PRDs are a great way to do that I found how do you usually work with AIs or MLMs Do you split it up Do you write out a list of requirements or do you just like YOLO it, as one would say? I think, yeah, I think everyone has like probably a different way that works for them. I have like two modes, like either like an exploratory mode where you're not really coding. You're just asking to come up with ideas and features and then more of a low-level mode. So I actually don't do exactly what you do, Kev. I don't really write those descriptions, but it's more like when I've decided that, okay, I want this feature. I've brainstormed it. Now I'm going to write a pretty low-level step-by-step thing. Maybe not on file level, but refactor this function, add this one, do it in this way that already exists in a similar structure in this other part of the app. So I'm kind of giving it a bit of guidance. And I see it more like you have the dough that is your source code. And then you're trying to basically mold it into, maybe dough was a bad idea, but maybe like clay. And you're trying to mold it in the shape that you want slowly with these different prompts rather than as it was before. You're just like on a much lower level updating one function at a time. And then at the end, you check like, okay, is it what I want? Yes or no. but now it's more like you're just shaping it over time to be what you want in the end. But if you're interested, I wanted to talk a little bit about how the actual Sveltebench works. Yeah, I was going to go get there. I was just interested in hearing how you worked with LLMs before because we got into that. So yeah, tell us about how it works. Because before I started this, I had only the vaguest idea of what the LLM benchmark even is. And it's actually quite interesting to, I've become much more skeptical of benchmarks. Like when I see a really good benchmark score now, now I'm thinking more like, okay, but what is it actually like testing? Like all these, some of these different benchmarks, for example, there are benchmarks where the LLM will write some piece of code, like a to-do app, and then another LLM will grade that code. And I'm like, is that a good benchmark? Like, I don't know. I guess it depends on how generous the grading LLM is, right? So I just mean a lot of these benchmarks are kind of like weird, like they're structured in a weird way. So I knew I wanted something that was like fairly reproducible like something that where you could run the same code multiple times and it would always come up with the same score So that why I hooked in Vtest But if you're doing like, so what the benchmark, what it is, it's like a bunch of prompts, like, okay, use the like state rune to make a to-do list. And then I ask it to like pre-fill some, create some like data tags on certain elements so that we later can test those with vTest. So the code comes in, we know that we check like, okay, there should be a button. When you click that button in vTest, there should be a new entry added to the to-do list. So we're doing kind of like proper unit testing. But the downside with that is you can't go like completely crazy with it. You can't tell it to like make a full to-do app, however it wants, and then like objectively test it. You need to kind of be a little bit fenced in and tell it like, okay add these test tags on these different elements so that we can actually test it properly but surprisingly that has worked fairly well because now we're at a point where where many LLMs score basically like basically have a perfect score on the benchmark so yeah I wasn't sure if it was actually going to work but over time the benchmark scores for for I think the top the top scoring one has 93.3%. So that's like, and that's on at 4.5 actually. So that one basically almost has a perfect score. It still messes up some things. And that's also something like LLMs are inherently like not deterministic. If you run the same prompt through, you're going to get different results every time. So I was reading up on how different benchmarks work. And I found this one paper by OpenAI, where they proposed this metric called pass at k. It sounds very complicated. All it really means is you run the test several times and then you basically check how many times did it actually succeed. So if you have pass at one, it means that you run it once, it works. Then you pass pass at one. If you do pass at 10, it's like you ran it 10 times, it worked at least one time. Oh, I see. Okay. So I'm just going to see here so that I'm not lying to you here. Yeah, exactly. So Pass-At-1 is the hardest kind of score to clear, and then Pass-At-10. We also have Pass in the benchmark And we do actually see that some LLMs they only succeed maybe 90 of the time or 70 of the time And that also like quite interesting data I think It's very interesting. I'm looking at the numbers here. And so just looking at Claude 4.5, you have like, it passes, like one version of Claude 4.5 passes almost everything except the inspect room. But then you have, I think, is Claude? No, that's Sonnet 4. Maybe I was thinking about. Oh, I see. Okay. Okay. So 4.5 is the latest one, I guess. Yeah. You can click, there is a button at the top that says V1 results because we changed the test a little bit to fix a bug. And then in the V1 results, you have a little bit more, some more models. But there, the inspect test is actually broken. So like it's no models could pass it. That's why the max score is 89% and not more than 90% because there's 10 tests in total. Okay. So how many models have you tested now? I know that every time there's a new open source model, I just come running and I say, Stanislav, you need to benchmark this. Yeah. I mean, I didn't know how fun it was going to be, but in the end we had, I was just like testing like new major models that came out because it's pretty easy. Once you have the benchmark, you just change the model parameter name and then you rerun it again and it gives you like a result that you can commit into the repo. But then I also had this guy, Max, come in recently and Max has been really good about testing models from Open Router. Open Router has a lot of like- Oh, that's nice. Open source model and like a lot of these Chinese models that don't have a big, they haven't really gotten a big foothold in the Western market, like Z.ai is one of those companies and Quen is another one. And those models are really good as well. And you can run them locally. Well, if you have a very good computer. What's been really cool is like, one thing that's been cool is to see how the closed source model have improved over time, especially Sonnet and Opus models, the anthropic ones. Gemini has also improved a lot. And then really recently, we actually got some of these open weights models actually scoring really, really well. So the second model now is KimiK2.

 which is actually an open weights model. Now, you can't run it locally because it is one terabyte in size. It needs one terabyte of graphics memory to run. But you could technically download it and run it locally. So, I mean, it's really cool. You could rent a couple of servers and then you could run it, right? And that means that you can ask it anything and you wouldn't have to worry about leaking data to all these providers and have them train on your code, right? and that's pretty big for a lot of companies, I'd say. And it's pretty interesting that we have an open weights one as the number two. And you're not seeing like, I don't see OpenAI. Well, okay, GPT-5 is number nine. Yeah, GPT, somehow, I don't know what it is with OpenAI. I think it's actually really, really sad. I actually think it probably has done a lot of damage to the Svelte ecosystem that OpenAI models, for some reason have been so terrible at Svelte 5. Like, they have been up until GPT-5, which was, like, fairly recent. They've all scored, like, basically at the bottom. They can't, like, you know, even if you tell them, like, write, you know, a state rune or, like, write a derived rune, they can't. They just, like, it's like they somehow... I mean, I know that it's a joke that LLM's write only React, but I think in OpenAI's case, I think they kind of over-tuned it for React because it just, and sometimes it's like insane. Sometimes you tell it to write Svelte and it will write you a React component and you're like, hello, guys, like I said Svelte. And it's like, oh, sorry. And then it will write like another React component. And you're just like, you know. Do you think that maybe like OpenAI is doing populist things like GPT has got to be the most generic, most accessible LLM ever? And so they're just like, yeah, everyone should usually react anyway because it's, you know, everyone will know it. I mean, in a sense, it would kind of make sense to train it for the most common use case, right? Because you need less training parameters and the model is probably smaller, so they can probably run it more cheaply. So if most people use React anyway, why waste time training it on Svelte? I don't know. I don't like that, but it kind of makes sense if they want to run it as cheaply as possible. And that's why I mean that they've done like a type of damage to the Svelte community because it is the default like all their models are the default models in like 99 of tools If you use Copilot you get GPT 4 which by the way is a terrible model It's like a tiny, cheap, awful model that can, like I think it passed, it is basically like at the bottom of the Sveltebench list, like it's all the way to the bottom, which is quite impressive, like worse than even some like 8 billion parameter models that you can run on a toaster, like on your phone, which is quite impressive. that it manages to be so bad somehow. And I mean, they have gotten better. Like I said, GPT-5 scores pretty well on the benchmarks. So it's not all bad, but it's taken them so long to get this working, which is a bit sad. Yeah, it's tough. So you mentioned this guy, Max, that has come in and done Open Router benchmarks. How do you look at the... So Open Router is interesting because you can find models that are not quantized and you can find models that are quantized. So do you pick the least quantized model or the most quantized? Because that's also something that you should probably present on the benchmark as well, right? And sometimes, like the provider that you get through OpenRouter doesn't even tell you what quantization. i've seen some benchmarks of is it the kimmy 2 or is it the zai one that's like if you run it through the official api it's much better than if you run it through open open router yeah it's this has been like a problem that we have i mean we're all very new at this we're trying to learn and understand open router is like a marketplace so if you have an open model you have a bunch of providers and they will all host it and they will like when you call the open router api it will basically give you like a round robin like whoever is cheapest right now or whoever has the least load right now and the what we noticed was that uh yeah like exactly like you mentioned the the open router version of kimmy was much worse than the official i don't remember if it was kimmy or it was another model but it was one of the big open source models and that's when we learned with max about this like quantization that some providers quantize some providers don't even like specify the quantization. Quantization basically means trimming off some of the precision of the model to make it fit into a smaller amount of memory which makes it cheaper to run but also worsens the performance So what we do now is that we try to choose either the official provider which we know that going to be the best because that's their official one. And if there isn't one available in OpenRouter, we try to choose the one that clearly states what is the precision level of the model. Usually it's like 8-bit, sometimes 16-bit. Yeah, that totally makes sense. and it's like you can't really do anything about the cases where they don't tell you that it's quantized but it is that's fraud basically yeah I mean yeah exactly we try to choose a single provider now when we run these tests so we will not get a mixture of different providers and it's actually sad also then because I think open models have gotten a worse score historically just because like they don't just have like one provider, they have many providers and it's harder to like verify the quality of the model. Yeah, that makes sense. So anything else about how this works? No, I think we covered it and it's pretty easy. Maybe we'll talk more about it in the future, but what we're thinking about now, like how to like improve Sveltebench. we're thinking about the version 2 with MCP support. Oh, interesting. Yeah, I mean, we are going to talk about the MCP, right? So I was going to ask, like, oh, what's next for Sveltebunch? But that's what's next for Sveltebunch. So speaking of the MCP, should we talk about the MCP? Does that make sense? Yeah, so Paolo... Has any of you tried it? Oh, I use it all the time, like with Claude. It's great. So I usually use Cloud Code when I do features. Most recently for the Svelte Society website, I've started implementing a social media scheduling poster in the admin dashboard. so the MCP has been great because it even if it's Sonnet 4.5 it still writes weird Svelte code sometimes even if it's it writes the one thing that I noticed is that it always writes the event handlers like on colon click rather than just on click That super common But with the MCP it just like auto fixes itself until it's actually correct. So that's very, very nice. But what does the MCP do? I mentioned auto fixing, but maybe we should just tell people, why should we use it? Yeah, I mentioned a little bit in the beginning that I was using the documentation to get better results, basically. So I started using this content JSON blob that was already available that had all the docs. And then later on, we got this like llms.txt format, which was proposed, which is like you basically merge your documentation into kind of like a markdown file and serve it at a known location like slash llms.txt. And we actually implemented that on the official site. So you can both download the whole documentation or you can on every single documentation page, you'll find a link to the llms.txt file for that documentation section. And that really, really helps. But, you know, LLMs have limited context window. So if you shove the documentation in every time, it does take up a lot of space and that costs money. You pay for every token. and it also like if you're using the like monthly plans you will run out of quota faster so the next step was like okay the documentation is quite big how do we optimize it to be smaller and to be more like contextual to what you're actually doing and around that time we started getting this mcp standard which is basically like it's like an api the llm can call sort of like it's the llm knows that I have this function. It's kind of like remote functions, but for a little bit. It's like, oh, I can call the list documentation sections tool and then I will get all the Svelte documentation sections. So what I did first was I started experimenting with this because I didn't even know if it was going to become something. I didn't even know if it made sense. So I made an open source MCP for Svelte, which had the documentation in it. So you could list all the documentation sections and you could fetch one or more documentation sections. And then, like, let's say, you could either ask your LLM to be like, oh, get the documentation for these things, or you could have the LLM, like...

 think about what would I probably need for this task and fetch those sections. And that worked pretty well. And then we started talking in the ambassador's chat about can we promote this to an official thing? And me and Paolo mainly, we started working on an official implementation. And Paolo, together with some of the other ambassadors and core committers, we're also thinking about autofixing, meaning if you submit your code that the LLM wrote to the MCP, you could get suggestions through static code analysis of things that the LLM did wrong. Like in your example, Kev, with the on colon click. So while on colon click isn't like a... It's kind of maybe hard to, unless you downloaded the event handler's documentation every time, you maybe wouldn't know that change, the LLM wouldn't know. But when you submit that code to the autofixer, the autofixer will return back, oh, hey, like you need to change on call and click to on click. And that is very actionable for the LLM. And that is a huge thing. And Paolo has done the main work on that. And he also, I think he uses the same kind of parsing that the normal Svelte build tool does. So it's all the same kind of errors that you would see in the problems tab in your VS Code tool. Oh, wow. Okay. He's using Acorn still, isn't he? Using Acorn to build the AST. Yeah, he's doing something. I don't know if it's Acorn specifically, but I know he had to extend some of the compiler tools so that they would be able to surface those messages in a programmatic way. So you could just send in a string of code and get back both the errors and warnings and even suggestions. So you could even suggest things like, oh, this effect you really should have used derived in this case, because I can see that you're just reassigning a variable. And this is huge. I'm saying it again. This is huge because you're getting a code base that is according to best practices. Yeah. Yeah, yeah. Actually, I have a question about this because the NCP is obviously an up-and-coming thing. Right now you've got these context things, but you've got two. You got Svelte Distilled which is a reduced version of the docs it looks like and then you got NLM Small which is a reduced version of the doc How does that work So I think the Distilled docs are currently only available on my unofficial version We're still talking about... Because even when you're fetching individual sections, it can still be quite a lot of documentation. So one of the things that I have been experimenting with is like, let's take each documentation section and then send them to an AI and say, like let's just shorten this down to the most important parts like leave the code examples leave the like most important like takeaways from the section but but like strip away everything else yeah okay and by doing that uh i have been doing that on my mcp for a while and and by doing that i saw that like you could reduce the document average documentation page by about 50 or 60 percent that's a lot because that's that's that's one thing that that i've met i've seen like when you're using the mcp you get some warnings sometimes from from cloud code that like this is a huge mcp call it takes 10 000 tokens something like that so i assume it's this to to reduce the the amount of characters yeah that's going to lead to less cost and and just like overall faster experience uh but one of the things that i think rightfully we have gotten feedback on from some of the core committers is is um okay but how do we actually know that the distilled documentation distills the correct parts and doesn't actually like leave things out or remove things that we actually need and that's where i think we're now talking about doing the svelte bench version 2 with mcp support because if we can test a like a version with the distilled docs and a version with the non-distilled docs and we can say that okay these are like almost the same like they are within like single digit percentage points of each other in terms of performance then i think we can pretty reliably and and scientifically say that like all the distilled docs actually like don't improve the sorry don't destroy the user experience like they actually have a value both because you still get the correct responses in the end yeah it's it's kind of funny like this it's it's almost like quantizing in a sense like you're taking the full text and then you're like stripping away precision, I guess, to see if it affects the output. It's very interesting. So the plan then for Sveltebench I guess we going back to talking a bit about that is to basically use the MCP and then compare the raw LLM performance try it with the non docs and then with the distilled docs But then there's the issue of every time the docs change, you have to redo the distilled docs, right? Or I guess the section, at least, that part of the docs, right? So then you would need to run the benchmark every time the docs update. Well, you wouldn't have to, but if you want the accurate. Yeah, I think usually when you have these benchmarks, you don't rerun them every time a new, like something changes like you usually do it every six months every year something like that i think uh the the question the big question now is like is is are the still docs worth it or is it worth like right yeah like do we really really lose a lot if we remove 50 of the docs or or do we lose almost nothing in that case it's totally worth it because it's just going to be better for for for users you reduce the docs yourself or are you using an ai model to reduce the docs yeah so it's an automated process so so um we send the docs to the um llm and say like hey can you like shorten this i mean it's just a prompt and then we get back the distilled like version of it so then are you going to try different right to distill the dots and then do you have to benchmark the docs distillation like who distills the docs the best like is that the benchmark you Yeah, now we're extremely meta-ing this thing. I mean, yeah, so far we're using Cloud Sonnet 4.5, and I think if we can show that the fall-off, the difference in performance between the full and still docs is basically not that big, then I think that's probably good enough, unless somebody really wants to try a different model. I think one of the things that I'm really excited for with this new version of the benchmark with MCP is to try out more, let's say, stupid models, like smaller models, because you have really great local models like Gemma 14B, 21B, stuff that you can run locally on your laptop Now it is too small to kind of like have the Svelte 5 in it I mean it just like that model is just 8 gigs or 10 gigs and it's supposed to have the whole world's knowledge, right? But it's really good at following instructions. So we think that like if we hook up the MCP to it, and it can actually like we can inject those extra documentation sections at runtime, we actually think that it might go from being pretty bad at Svelte 5 to being pretty good at Svelte 5. So that's what I'm most excited for, to try some of these even local models that you can run yourself, because then you could actually, for no cost, use a model that works pretty well with Svelte 5. Because right now there is no local model that works really well with Svelte 5. But I feel like it can be. And it's very, to be fair, it's very expensive to use LLMs at the moment. For example, Claude Code, I think the cheapest plan is... Actually, I guess you can use it with Pro now, but it used to be €90 a month. And that's out of range for most people in the world, unfortunately. I don't pay that for Claude Code. I pay not much for Claude Code. Yeah, probably €20. euros i think they changed it so you can now do it but but when they first released claude code needed to have like the max something and that costs 90 so so the cheaper gets the better i don't know if you followed along with the claude anathropic thing but recently they've like tightened the limits quite a lot so there was a there was an article i think it was by ed ed zitron and he had looked into some of the like financial data and found out that the anthropics AWS bill is like higher than their actual income like the actual revenue and I think we kind of know this that models right now are being heavily subsidized like the thing that I think Google is the best slash worst offender at this depends on how you count it because they have this like AI studio and you get a bunch of free calls every day and that is just completely unsustainable right But they train on your data, so they feel maybe like it's worth it to them. And then OpenAI is the second biggest subsidiary because you get so much codex usage and so much OpenAI usage for 20 bucks a month that it's totally untenable. And Claude is the first one, or Arthropic is the first one that has kind of scaled this back now in this past month.

 to something that I think is more reflects the actual cost for them. So some people are saying that they cut the allowances by 60%, 70% versus what they were before. And a lot of people are very unhappy about that. Yeah, especially if you're getting billed the same amount of money. But I think the thing is, they should go with the sort of gym subscription model whereby everyone pays and then never goes because they feel good about having a subscription. If you think about it, someone like me who barely really uses Claude, but I still pay the subscription every month, I'm subsidizing somebody else and if they had enough if they make it cheap enough for a basic plan monthly that enough people like me will just subscribe just in case I need it then it means that maybe they can subsidize all the really heavy users and then I think it's a basic business thing rather than actual and not a problem with AI per se Yeah, I think it's also kind of an unknown with how much it costs to train the models right? Yeah I don't know how you realistically build for it as well, it's quite difficult it's quite tricky, everything's a bit bespoke isn't it? I guess it's one of these things that will improve over time, there'll be a company somewhere that comes and does billing consultancy around AI models Like we have AWS billing engineers We have this right, there's so many I get contacted daily by cloud optimization engineers, who are basically people who come in and look at your cloud billing and go you're spending too much here and here. There's no reason that can't be done for AI, but it's a pretty new market. I mean, I don't. I spend nearly nothing on the cloud because I'm tight. Have you guys seen my recent rage at the serverless? I've been crazy on Blue Sky posting benchmarks and stuff. What's the latest? So the latest is obviously the AWS East one went down, right? Half the internet stopped working. Something like that uh but then there there was this benchmark that i think the theo did uh on so the the twitch streamer he did a stream and a benchmark where he compared versell and cloudflare and like the how fast they were uh basically doing some some work and he showed like oversell is three times faster and then cloudflare came out with an article oh we we fixed a bunch of this stuff blah blah blah And then Railway I think they like a VPS host right They released an article a couple of days ago I think where they did the same benchmark and then showed Railway VPS is doing it for like one third of the, like three times faster than Vercel and Cloudflare. So that was the latest thing that I saw. I mean, my Vercel bill's gone up because I switched to the new ProPlan from the old ProPlan. There you go. And it's like, I feel like it's really gone up a lot. and they've seen it, they said it'd be the same, so I'm not sure I feel about that, to be honest. I think we can have probably a whole episode about serverless if we didn't already, because most of them are just reselling AWS or something anyway, so they have those same. That's why, if we take it back to AI, that's why all of these, like, what is cursor and others are not really profitable, and why the cursor has also worsened their models, not their models, but they've worsened their allowances a lot, because Cursor is paying for Cloud or Gemini, and that has a fixed cost. Like, they don't really give, from what I heard, they don't really give big discounts, even if you have huge volume, because probably they can't really afford to give a huge discount on the inference. Also, like, you can... No, go ahead, sorry. I'm just saying, if you're selling, like, what Copilot had before they nerfed... Sorry, not Copilot, what Cursor had before they nerfed anything, which was basically, like, you could run a model continuously, but just a little bit slowly. And you're billing someone $20 for that and they're using Sonnet 4.5, which can accrue thousands of dollars in API costs. That's not a model that can ever work. Yeah, exactly. Yeah. I was going to say, it's not only the limit, like when you're talking about agents, like it's not only the LLM that is, well, like you can use, maybe the actual agent uses Claude Haiku that's the cheap model that does the planning and then that one calls Sonnet when it's doing coding so that's another way they can cheap out on like costs so you can get and Haiku is a worse model than Sonnet so you would get a decreasing performance and you wouldn't be super happy about that either, even if the limitations are the same. What you want to do is call out to GitHub and have a look at this project where it called Spelt Benchmark and then find out what the best model to use is and then use that model Like a router performance It interesting Like I saw recently there this thing called ACP which is like agent, what was it? Content protocol? Agent communication protocol. Oh. So that's like a protocol to connect different agents. So you could imagine you had like a planning agent using one LLM LLM and then you had like a Svelte agent that you could host somewhere and sell to someone and then that would be optimized only for writing Svelte code and then sell it as a team of engineers call it outsourcing you could do that sell it on Upwork I think that's really exciting I know there's like a joke that there are more MCP developers than there are users and I think that joke is not quite true but almost Yeah, you posted a link recently. I think that we see, it's not a huge amount of MCP usage yet, but I think everything is going in that direction with the agentic tools and more long-running processes that present a whole PR for you rather than just coding in one file or one function. so i i do think something like business ideas like that are probably going to be really big like i think like an outsourced uh svelte ai developer that has like the docs and and cool mcps and other proprietary process for smartly routing to different models to keep the costs down that is a business idea that is maybe not for today but for in a year or two years or three years from now. Yeah, because if you manage to use one of the open source models then you can host it yourself and you can get really, like you can get away pretty cheap, cheaply compared to like having to call Anthropic and Cloud 4.5. So we've mentioned the autofixer, we've mentioned like the documentation but there's also like a playground thing. I haven't seen that one before. I just saw it in the docs. So this is, I'm talking about the tools that the MCP has access to. So what is that? That's also something Paolo suggested, which is really nice. So you can basically ask the LLM to create a Svelte playground and it will generate like a link You click on the link and you can see the component that the LLM wrote for you directly in your web browser And just literally yesterday, the day before yesterday, Paolo implemented a new feature that came to MCP where you can actually see the result kind of inline in your client. Now, if you use something like Cloud Desktop, they have this artifacts feature where it can write code and also run simple websites for you directly in cloud. You can make little calculators or little games or something like that. But apparently now that's becoming sort of like an open standard in MCP. And as far as I know, there's no clients that actually support it yet. But the Svelte MCP supports it. Nice. We're ready for the future. Yeah. So when we will get, maybe cloud code will one day support it and then you'll be able to say like, oh, write like a little map tool for me in Svelte 5 but the Svelte MCP will actually be able to show you a preview of that map tool directly in your cloud desktop or cloud.ai, like the website. Yeah. Yeah, I think this UI thing is, I think Shopify is a big proponent for this. So I think what they want to do from Shopify's perspective is that they want to let stores kind of show up in line in your, like, for example, Claude application on the desktop, right, where you can actually buy stuff. So it would show up with the store. You would maybe search for, I don't know, I don't know, a mini PC. And then the store would show up there and then you could just click buy and it would buy it. but it would really just be the mini PC store running somewhere else. So I think that could be a future as well, where we don't actually surf the web. We're just stuck in these AI desktop applications. You never know. It's really funny. It's like they're trying to shove products in from two different directions. So either you get it through your AI client, or now that like open AI launched a browser so that they can just like surf to the products that you want to buy and buy them for you. But it's like someone said about this browser, like part of the fun of actually buying a product is actually kind of looking at,

 online and like figuring out like is this for me you don't just like want the like an LLM to just like buy me a tv like okay what what tv am I getting why am I getting this tv like yeah should I just say buy a tv and it just shows up at the door no I'm not going to do that I saw a tweet like that but about like travels because a lot of the demos they have is like oh like you book me like a travel but I'm like don't you want to like look at where you want to go like do you really because like when you're planning a travel you're looking you're getting you know inspired you don't just want to like tell you like oh i booked you like a two-week holiday to greece in three months like is that what you want is that what people want i'm not sure yeah i don't know maybe anthony are you one of those people want anthony do you want the lm to just say you're just gonna say make me uh like find me a trip to greece and it just like go no not even greece just find i i'm going on a two-week holiday book something for me and it's just like goes away and does it you know i think about this a lot right and i and i was saying maybe a few episodes ago that i'm i'm using um and one of these ais to basically pick up linear tickets complete them open a pr and i just approve it and merge yeah and the one thing i've learned from it is that actually i want more human interaction one thing because i want it to like pick up a ticket and then ask me questions about the task before it starts and makes loads of assumptions and so from my perspective actually what i want to do is i want to say let's book a holiday together and it goes and finds a bunch of good options for me and i can look at them manually and assess them and then and i want that whole like back and forth in it and i think there's something that i don't know i don't know if it's been nailed well or or not yet i don't know if i don't know if this is the answer you wanted like whether you want a silly answer or a good answer but this is this is actually a feeling i have every day is is how can this be more like suitable for somebody to me who has very much distrust in lms as a whole yeah i think that the cloud code must have listened or the atropic people must have listened to that episode because they just added like question asking to cloud code i don't have time to see it yet because if you ask it to do something it will actually like sometimes ask you questions first it's like oh do you like would you like to refactor all of the usages or just the small usage. And actually, those questions are pretty good. The times I gotten them in the flow it actually asks really really good questions Are you sure you don want to change it all to React Are you sure Well, speaking of the LLM asking questions, it's one of those things where at least Claude Sonnet is not very good at telling you this is not a good idea. It's just like, certainly, let's improve this. it's a great fantastic idea and you're like I don't know about this is this really a good idea like I want you to if it's not a good idea tell me that it's not a good idea and it's not very good at that maybe like there's prompt engineering to solve this probably but I wish I would just do it without me having to write a special prompt for it but it is what it is so um and any anything else about the mcp that we haven't talked about that we should talk about well i mean we're always looking for um we're always looking for feedback like one of the things is that we don't really we don't really have a way mcp doesn't really have a way to like you know determine whether like it works well or not so i think that's something sometimes we see like on Twitter or on BlueSky, people are posting that they have specific problems. Like some people have posted about the, that they are getting these big responses and that's something we're looking into now. So if you're using AI, I would encourage you to try the Svelte MCP and if something is not working, create an issue on Discord in the ecosystem channel or just tweet at us so that we can be aware of it because we don't really have any good observability tools right now to see like, oh, this call totally didn't like do what it was supposed to, because there is no such like feedback mechanism at MCP. We can only say that the user asked for, for like this tool or that tool. And we can only get very generic like information about the, the calls. Yeah, that's tough. Like getting, getting actionable analytics from, from this. And it's not like you want to save all the API calls with all the people's code and stuff like this. But that's important also to mention that there is actually two versions of the MCP. There is the remote one which runs in the official Svelte infrastructure But if you for some reason we don obviously save anything and the code is open source, so you can inspect it yourself. But if you still don't like that, you can also run it as an MPX command, run it completely locally with all the same docs and all the same autofixers, but just running on your own computer, if that works better for your workflow. But for most people, just putting in a URL to a remote server is usually much easier than running an mpx command or having another terminal up or something like that. So it's kind of like a convenience thing. I just realized, we have talked about this, the Sveltebench and the MCP, and you introduced yourself. But I'm not sure you mentioned that you actually run a YouTube channel as well. Right? Do I get to pitch my YouTube channel? Absolutely. It's mostly about Svelte and AI. So I think that makes sense. Well, I'd love to pitch it. It's called the Code with Stanislav. You can find it if you search for Code with Stanislav or if you often, hopefully, if you search for Svelte adjacent topics, like I recently put out a video about internationalizing Svelte apps with this new cool internationalization library called... I'm not going to try to pronounce it, actually. You're going to have to search for Svelte internalization. I got feedback from the author about the pronunciation, and I actually don't remember how it was supposed to be pronounced. But you can check that out. And I have another video coming out in a week or two, which is actually going to be only about AI and full context coding, which I think is going to be very interesting. Exciting, exciting. So we can expect more videos. We can expect better Sveltebench, better MCP. I mean, it's already really good, but the future is bright, is what I am hearing. That's great. Yeah, we're at a level now where the experience with Svelte 5 is really, really good, I would say, with AI and Svelte 5. And it's like if... This is the worst it'll ever be. It's only going to get better. Yeah. And if you want to use it, it's simply just adding an MCP in most of your AI tools and then you ready to go All right I think with that said we can move on to the next section unpopular opinions This is an exciting section You don have to have an unpopular opinion Anthony always has one. Well, not true at all. But I do, because I did write one down just at the start of this podcast, because I was thinking about it. I can go first if you guys don't have one. Yeah. And I'm not sure in today's environment that it's unpopular, but serverless is overrated. I can say this as a self-hoster with a mini PC on a shelf that runs the Svelte Society website. It's not as hard as you would think it is to run your own stuff. Yeah, I did it when I was 14, Kev, but how happens when you get hacked? you have Cloudflare in front so you are using serverless so you partially self-host I partially self-host by the way I mean you would have a CDN in front of it anyway right so okay interesting I get Cloudflare to self-host for me by the way I feel like we need an episode if we didn't do another idea about the self-hosting SvelteKit it's a good idea I might actually, let's do that. That's good. But yeah, serverless is expensive. You don't need it unless you're like a huge, huge company that has millions and millions of visitors. Like even the tiniest of servers should be able to manage like 50,000 users, I would hope. But people are used to getting the cloud engineers selling them on all these weird products. yeah i wouldn't like to serve i wouldn't like to serve beyond from my own connection let's put it no obviously serverless doesn't have a lot of favors yeah no no i totally get it like but it's it's that old uh like saying that use the right tool for the for the job right yeah it's like the the the svelte society website if that goes down for an hour no one is gonna like yeah lose their lose their brains over it like it's it's gonna be fine people might not be able to search for the packages that they need at that specific moment. And I feel like that's a good, that's an okay downside to it. Yeah. It would be like during the AWS demo.

 downtime, it was up. Everyone could search for their packages. But then they couldn't do any work anyway because Slack goes down or whatever they're using. But not the Cloudflare downtime. Because if you're the Cloudflare downtime, it wouldn't work. Correct. Because you don't self-host, by the way. Fair enough. Wow, this is very, like, thorny. I hear that there's some resistance here. That's why they call it unpopular opinions. in my defense you know i started biancon on now back in the day when before it became the sell and i did that because now was impossibly cheap before what it did it ran everything at scale it was great and you know at the point where it became everyone's saying it's expensive we optimize some things and actually now it still isn't expensive for the amount of load we take and stuff so i'm pretty happy with it in a general in a general sense i did say though recently yeah the bill i think i mean it could be just the seasonality but i think the bill increased quite a lot well that's that's the thing right like if if you had a dedicated server or something you you would it wouldn't change like the bill would just be the same as long as you over provision it would be fine but that comes with other other traffic wise we might be paying more for traffic because you always you're always going to have some bottlenecks on contention and I think it would be traffic we end up paying for. Yeah, I don't know the statistics. You would know you run it, right? Yeah. Well, yeah, I mean, the traffic bill on other things would be high anyway, so I don't think it's expensive what it is still. But then, again, I spent a lot of time optimizing it. There are things that if I just let them roll would have been really expensive because edge functions are expensive to run. The quotas are quite small. the bandwidth quota is actually quite small we pay a few hundred dollars for it there are things I've done in order to keep it low because I like the DX of the cell it is very slick it is so that's my that's my unpopular opinion do you guys have one? I have one, do you have one Stanislav? yeah I have one ok, go for it alright, so my unpopular opinion to keep it on topic is that I think OpenAI has the worst models out of the three big providers. With Svelte we can now say with 100 scientific accuracy that they have the worst models but I think also for general purpose task I just like constantly underwhelmed or rather overwhelmed with Entropic and Gemini which I think are better so it's it's it's sad that the worst company is the one that is like synonymous with AI so that's kind of yeah that's kind of how they do it though isn't it they keep it super generic I suppose and and and that has a price than most, you know, skilled people. Do you think that the GPT-5 thing was, so they released GPT-5 and their conspiracy is about how GPT-5 is smaller. So it's cheaper. And that's one of the reasons why it's getting worse. Or maybe the other models are improving. Yeah. If you want to know how big a model is, look at the API pricing, because that tells you the truth. If GPT-5 is cheaper than GPT-4.5, then it is smaller because otherwise they wouldn't price it lower because they don't have any incentive to... They have no incentive to subsidize API usage because that's mostly enterprise and corporate use. Right. No, that's true. Most regular people just use the $20 plan or whatever. Yeah, that makes sense. All right. Anthony? Right. My unpopular opinion is not on topic at all, as usual. That's fine. Heat pumps are bad. Heat pumps are bad, actually. Now, you're be surprised. What kind of heat pump? I mean a heat pump that heats a house. Like an air heat pump? Like an air source heat pump. Any kind of heat pump at all. I mean, I'm saying that because it's not like me to say that at all. I don't think they're bad, really. but they're bad in the context of i was reading recently about lithuania uh and all their new builds that they're building in lithuania or i imagine you know the ones that they're they're talking about um are being built without heat pumps they're actually using electric underfloor and the reason they're doing it is clever is because they're actually because they're building passive house standard houses which are going to insulate so much that you barely need to heat it in the first place that means that it's actually real overkill to have a heat pump because you've got all the maintenance needed, you know, you've got the servicing, you've got all the water running around your pipes, things like that. Putting electric heating mat in that barely used you know just a little bit of latent heat and the house just holds it all in is like zero maintenance it just kind of work forever and ever and ever and it a way better when every other country is going right heat pumps heat pumps they're going screw that electric under floor solar panels problem solved and i think that's really nice for new builds i think it's really nice and the uk could learn a lot we're so behind in all this stuff we're still doing gas boilers and two solar panels that are not going to help anyone so i would hope that we can learn from this but lithuania really like pushing with that so yeah my unpopular opinion is is more like more like clickbait you know uh uh so should i feel bad for buying a house with a heat pump no because i won't i i spent a lot of money getting unfitted and they're brilliant they are wonderful for an existing house yeah yeah do you the your like how to say your unpopular take sounds like a technology connections video do you know channel yeah because he always he's always talking about like how ventilation is bad or like some type of refrigerator some type of solar is bad like yeah everything is everything is bad did you see the technology it's also like one of these technology places where he builds lasers and does like his own liquor and stuff and he recently did a video i'm gonna see what it what the channel is called it sounds like colin furs who who built an underground no no so this is house this is another that's a different kind that's more like entertainment right yeah yeah maybe i'm actually thinking about technology connections no no but he doesn't build no necessarily no dissects this guy just like he but he builds like a like a sound gun and stuff and like test this on tested on people making your own heat pump sorry now i'm just searching so we could we could also i need to find this he's gonna he's gonna find it i'm gonna find it i'm gonna find it don't worry we cut this bit we'll cut this bit out we won't yeah we won't tech ingredients is what it's called tech ingredient it's very very nice channel super super super nice uh uh so why i mentioned the channel is he recently did a video where he let see here he takes uh an ac regular ac unit and converts it to geothermal unit by by connecting pipes to it and just digging a hole and putting the pipes in Do you know, this popped up on my YouTube feed last night, I think. Oh, there you go. Because I was going to watch that. That sounds good. Yeah, I was going to watch that. It is good. It is good. I don't think I'm far down in the rabbit hole yet. I'm a little bit scared. Don't stand by the edge because you will fall in. It's like one video, he does everything, right? He's like, oh, one video he's like brewing sake. And another he's like laser welding. And then he's like, oh, we're building our own e-bike. And he does it like from scratch, from scratch. I would love to have that kind of time. Yeah, yeah. Oh, here's a tornado chamber. Who doesn't want to build a tornado chamber, right? I don't even know what that is. Sorry, tangent. Okay, I think that's her unpopular opinions. now it's the last section the picks the picks, these are always hard these are always hard do you have picks? what are we picking? you can pick whatever you want something that's technology connected a game, a book an NPM package something that you made yourself that you want to show anything really so I guess I will pick tech ingredients as my pick. That seems fair. Yeah, I can go. So I've been a little bit obsessed here with my fiancé. We'll be playing Ball Pit, which is one of those new games that kind of sucks you in. It's kind of like an Arkanoid old school flipper type thing, but it also has meta progression and the roguelike elements it's got like all the buzzwords very fun game nice wow this looks wild it's like a it's like a mobile game but there's no enough purchase so you are free to just completely get sucked in without worrying that they're gonna take all your money interesting okay for now for now um okay my pick is kind of lame i mean it's just something that i have and i'm enjoying it so I went recently to a beach near my house in the UK

 which is called, what's it called? I can't remember. Something Cove. But it has saunas on the edge of the beach. And you go in the sauna, and then you run into the sea, and you swim, and then you run back into the sauna, and you keep going back and forth for that. Sounds very Scandinavian, actually. It sounds very Scandinavian. I'm guessing it's probably based on a Scandinavian thing. I've never done that, and it was great fun. I really enjoyed it. Did you drink beer in the sauna as well? No, no, no. That's the next step up. It was like 10 in the morning or something. So we also just did it for that reason. That makes sense. Yeah, well, most of that makes sense. But yeah, we did this and it was great fun. And the thing for me was the most amazing of all was how relaxed I felt afterwards. You know, just going hot to cold, hot to cold all the time makes you feel really relaxed. And so I went in the car. I wasn't driving, luckily. I was being driven back to my house and I couldn't even stay awake. I was so relaxed. I was just kind of like almost passing out. That sounds dangerous. well again i wasn't driving luckily um but uh i was thinking about it and i think right i've got because i've got a jacuzzi in my house right this isn't like my brag brag brag cast here but i have a jacuzzi in the garden and it's really cool because it's now winter and it's freezing cold and you get a jacuzzi it's really hot and i thought what i need to tip this over the edge really is i need to buy a plunge pool so i went to amazon and i bought one of these 30 quid that's like a massive bucket that you fill with cold water and now i jump in that and then i jump in the jacuzzi then i jump out to get like a hot frog and then back in there and i jump between the two and you get that same i get the same feeling of relaxation when i'm out when i'm out it's like that's amazing and it also is weird because once you've been in the really cold pool you can you could like the cold weather outside doesn't count anymore which i've not experienced either like you don't tell it's cold i could just stand outside in the garden wearing any swim shorts dripping wet and being like i'm actually quite warm now and that was quite shocking to me that's quite amazing So I kind of obsessed and it was only 30 quid So I thought well you know what that not bad That not bad That good That good I actually found a pick so I going to switch my tech ingredients pick from that to You're getting two picks. I mean... Well, he is the host, so... It's true, it's true. I mean, Anthony is also a host, so... Well, I know, but Kev's the master host. Kev's the master host. Oh, I see. Well, so, Hibachi Grills. Have you heard about them? Oh, yes. I recently bought one. So it's like this Japanese-style barbecue where you put coals in and then you fry skewers and stuff on it. And it goes up to like 1,200 degrees Celsius, which is insane for like a barbecue. And I bought it. I'm going to test it out tomorrow for the first time. So I guess it's not really a pick yet. We'll reevaluate next week probably on the show, see if it's any good. so that's my pick but yeah I think that's it for the show right unless you guys have something else to mention where can people find you online Stanislav good question I actually made a new website which has a pronounceable URL it's stanislav.garden so if you go there if you go there you'll find my digital garden with all of my cool links All right, I'll put that in the show notes. And you are on Blue Sky, Twitter, GitHub, and all the other places. And you can find links on there. I can see that. Even LinkedIn. It's very, very enterprising of you. How professional of you. Very professional. Yeah. All right. And with that said, thank you all for joining us. Thank you, Stanislav, for coming on and talking about Sveltebench and MCPs. It was a lot of fun. and yeah thank you to everyone who's listening and we will talk to you next week, goodbye yeah bye