 Svelte is an amazing way to build web applications, but with the advent of AI coding, many people, including myself, have found that LLMs aren't as good at writing Svelte code compared to some bigger frameworks out there like React. And with the introduction of runes in Svelte 5, we saw Svelte syntax change quite a lot, which made LLMs even worse at writing Svelte code. The models would often fall back to using the old Svelte 4 syntax or hallucinate how they thought runes should work even though they had no idea due to their training cutoff being sometimes years before Svelte 5 was even released. Since I code with Svelte every day, I've been doing my best to work around these issues somehow. I started with creating an implementation of the emerging llm.txt format for Svelte and SvelteKit. With these files you could get the entire Svelte and SvelteKit documentation as a single text file and add it to your AI conversations. This helps resolve the most egregious errors that LLMs would make, but since the Svelte documentation is quite large, it also eats into your context window when you use these files. This can be quite expensive or cause you to get rate limited by the LLM provider that you're using. Since creating the initial Svelte LLM site, the same logic has actually been integrated into the official Svelte.dev site, meaning we now have official versions of the LLMs.txt files moving forward. My next idea was to build a dedicated LLM benchmark for Svelte 5. You might have heard of benchmarks used in the context of testing testing the performance of new models, and there are benchmarks out there for anything from general problem solving to creative writing to advanced mathematics. So I thought, why not make one specifically for Svelte 5? I was inspired by this one paper by OpenAI where they proposed a way of testing how well models can write code specifically In short the Sveltebench benchmark I created prompts the AI to solve problems using Svelte 5 and is focused on how to correctly use different runes like state and derive. In this screenshot, you can see the test we used to see if the AI understands the derive.by rune. The AI then writes a solution to the problem, and we run a vtest test suite against the generated component to check that it actually works as intended. If this sounds interesting to you, I plan on making a whole video on how I built Svelte's bench, going into all the nitty-gritty details of building an LLM benchmark. So if that sounds interesting to you, don't forget to subscribe to see that video. I've been running this benchmark for a few months now, continuously updating it as new models have come out. And in this video, I thought we would focus on the results that we've seen so far. I'll go over the three largest LLM providers, OpenAI, Google, and Anthropic, and we'll see how their frontier models do with writing Svelte code as of today. Let's start with OpenAI. Out of the three big providers, OpenAI has the most number of models, yet still, somehow they have the worst results in our benchmarks. Looking at how the current ChatGPT model does, we see that it struggles a lot with writing Svelte 5 code, with only the simplest Hello World test passing, and with most other tests either failing outright or passing occasionally while failing most of the time. The same is true for the base GPT 4.0 model that ChatGPT is based on. Next, let's look at the the 4.1 series of models. Starting out with the largest 4.1 model, this is the one I had the most hope in since it's one of their newest models, yet we see that it does pretty much just as bad as the 4.0 series. And this is also true for the smaller models, with GPT 4 mini and nano getting similar results What this shows us is that none of the 4 series of models can write even the most basic Svelte 5 code without something like an llamps file to steer it correctly Next let move to the O3 series of reasoning models, starting with the largest O3 model. Initially, it does look like it passes slightly more tests than the 4.1 and 4.0 series of model, but once we drill into the data we see that it frequently writes incorrect code. For example, for our test that asks the model to write a simple counter component, O3 succeeded only one out of ten times, and as expected O3 Mini inherits this poor result. And finally we have the O4 Mini model. Honestly, the OpenAI naming is kind of crazy, but this model is apparently a newer generation of the O3 Mini reasoning model. There's not really much to say here as the results are just as bad or worse compared to the O3 models. In summary, as of the making of this video, OpenAI has zero models that can write even the most basic Svelte 5 code. I look forward to continue testing new OpenAI models and I hope to release one capable of writing Svelte 5 soon. Next, let's look at Google. Google has been a bit of an underdog in the coding space and most of their models have frankly not been very impressive so far. But that changed with Gemini 2.5 Pro that for a short time was the most highly praised coding model. Starting with the Gemini 2.5 Flash preview, it's a bit of a mixed bag. While this model does better than every OpenAI model, it still frequently writes incorrect Svelte 5 code. Moving on to Gemini 2.5 Pro is where we get a big surprise. Gemini 2.5 Pro is actually very capable of writing Svelte 5 code. When I tested this model a few weeks back, it really felt like a huge moment to see any model be able to write Svelte 5 code reliably using runes. I'm really looking forward to seeing how well this model works out once it out of preview Finally let look at Anthropi Looking at Cloud Sonnet 3 it was in this sort of twilight zone where it could write some Svelte 5 code flawlessly passing several benchmarks like the derived and effect ones, but it also failed really hard at some specific benchmarks like the snippet benchmark test. Moving on to Sonnet 4 that was just released the other day, it's a different picture. This is the first model that I would say got more or less a perfect score in the benchmark. erased all the tests except our inspect test. And the reason for that is probably that the inspect room isn't used very much in production code, so there's just less to learn from for a LLM model. As far as I'm concerned, this is the first model that was truly capable of writing 12.5 code. Moving on to Sonnet's big brother Opus, the same is true there, getting a perfect score except for the inspect test. I think Anthropic really cooked with Sonnet and Opus 4, and if you have a choice, I would say that it's by far the best model to use today. And of course you can still use the llm.txt file in case you run into some edge cases that the models still can handle, or something along those lines. When I started looking into LLM support for Svelte 5, I wasn't sure how long it would take for models to support Svelte 5's new syntax. And I'm impressed that it only took about seven months since the release of Svelte 5 in October of 2024 for models to be able to write Svelte 5 code. That might seem like an eternity in the AI world, but all things considered, it's very fast and impressive. I do have some other things planned around AI and Sveltefy, specifically around training local models, and so if that sounds interesting to you, don't forget to subscribe to see that video first. I'd also love to hear your thoughts on this, which models have you had the most luck with, and what are your tips and tricks to get the most out of the existing models we have for riding Sveltefy. Leave a comment with your tips, and don't forget to press like and subscribe if you like this video. See you in the next one.