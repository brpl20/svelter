 If you've been using LLMs like ChatGPT, Cloud or Gemini for writing code, then you know how frustrating it can be when the AI doesn't understand you, doesn't give you the right way to implement things, or keeps giving you incomplete code. This is not only a waste of time, but also a waste of tokens, which can get really expensive. I've been programming with LLMs since the early ChatGPT days, and just these past couple of months is the first time that I feel like models have progressed to the point that you can actually get very consistent performance, and I've also picked up a few tips and tricks on how you can push them as far as possible. Today I'll share my top 5 tips on how you can improve LLM performance, so let's get into the video. So my first tip is to provide the model with your existing code. And I don't just mean provide it with like a snippet of your code, I mean try to get the whole codebase in there. Now when you do that, that really improves the performance of the code the LLM generates. It does a lot of things that you wouldn't even think about. For example, it helps to match your coding style. So it's going to know how to implement certain functionality. Like for example, you'll know all of your code patterns. How do you do database access? What sort of libraries are installed? What sort of packages are you usually using to solve problems? So when you ask the LLM to solve something that has already been done maybe once or twice in your code base, it's going to give you the result right away. and it's going to be correct. You're not gonna have to keep re-prompting it. Now I have a separate video on my favorite way to include your entire codebase into an LLM for use in this way and I gonna put the card up there if you want to check it out Now my second tip is to include the documentation for the libraries you're using. The base LLMs they do know a lot of stuff. They probably know the packages you're using but every LLM has a cutoff date and cutoff dates means that the LLM stops learning at that point. So while the LLM might have the documentation for an earlier version of the library you're using or the libraries you're using, they might have been updated. The APIs might have changed. There might be a completely new version. And then the LLM basically doesn't know what to do. And that's going to lead to it giving you old code that doesn't work anymore. To solve this, a lot of sites have started to provide this LLM.txt file. Even if the library you're using doesn't have that, you can usually still get a download of the documentation using Markdown or a similar text format from their GitHub repository. Now, what I usually do is I include these libraries, documentation, into my project. I even put them in version control so I can keep track of which versions of the documentations I have. And by using the previous tip of including your codebase, you can also put all the documentation into the LLM the same way. Now, my third tip is to have a strong starting prompt. Now, I'm not going to get into prompt engineering, but I can give you a few tips that has helped me where I have seen that actually putting something in a prompt actually improves the performance of the code it generates. So first I usually in my prompts tell the AI that it's an quote-unquote expert developer. I know it's silly but it works. Second thing I tell it is to always include the full files This even though it not perfect it minimizes the AI trying to give you just like snippets and instead it always gives you the full file back or at least as often as possible I also tell it to preserve comments because I noticed that AIs are really good at removing comments in the codebase. The third thing I tell it is to use the entire project context. Basically read the project carefully before answering. The fourth thing is I tell it which libraries I prefer. Now, if you include all of your libraries and examples on how you use them, as in the previous tips, this should work more or less out of the box. But still, sometimes it can be good to tell that I prefer this one library over another library. And the last thing is to put in any project-specific needs. Like you might have constraints on some packages that you can't use, or you might have some performance budgets or some other technical limitations. Make sure to include that. Tip four is to let the model actually think. When I prompt, I usually don't just ask for code. I usually ask the model to think a little bit using a technique called chain of thought. And I have a video on that that goes into the detail a little bit on how you can set that up if you're interested in that. Basically, instead of telling it, like, just write the code and don't give me anything else. I actually ask it to think first, to do, like, an outline of the changes it wants to do and then write the code. And, okay, this might seem, like, annoying. And I know that models yapping is, like, a thing that people don't like, but it makes a difference. and with these new reasoning models that have come out, I think it basically proves that if a model gets to think a little bit before it answers, it actually makes a huge difference for the output. What you basically doing is you asking the model to talk and converge on the actual output that you want and that is much better than just saying like give me the code and then it just gives you the code And my last tip is to use the right model for the job Just recently we had a few of these so reasoning models be released. We had the OpenAI 01 and just recently we had the DeepSeq R1. So I usually split this up into two different pieces. So for kind of incremental tasks like writing a small function or implementing like some UI, I usually use a strong base model. For example, Sonnet 3.5 which is my favorite now, but the Chatshift 4.0 is also an excellent model for this purpose. These models, they're quite good also at doing architectural stuff. They can ask you to architect an application and it's gonna come up with a good suggestion. Sometimes it's not enough and in those cases I sometimes reach out and try reasoning model. Now reasoning models, you can't really use them for like day-to-day coding, it's too expensive. I think only get like a few dozen messages per week or per month depends a little bit on the model right but it's very expensive so I usually only save these for those cases where I tried my normal strong models and they just can't get the job done and the last thing I want to say is just don't expect too much even from the reasoning models how I think about it is that if I'm trying to do something and my normal model can't solve it and the reasoning model can't solve it I'm probably going too fast and I'm not really thinking about what I want to do so in that case I usually try to take a step back and try to think about how I divide the problem up into multiple pieces. Sometimes I ask the AI to do it and sometimes I do it myself and then I try to do it more incrementally. So those are my top five tips for improving LLM performance. I hope you learned something today. If you have any tips that I didn't bring up today please share them in the comments and have a wonderful day and don't forget to like and subscribe.