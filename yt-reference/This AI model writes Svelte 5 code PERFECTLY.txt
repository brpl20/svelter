 If you've been using AI to assist you with Svelte coding then you've probably noticed that getting AI to write Svelte code that is up to date and with all the latest features and best practices is no easy task, especially since the release of Svelte 5 with the new runes-based reactivity system and differences in syntax. I was having a lot of problems with this and I also heard from many of my friends in the Svelte community that they were also struggling. But I also noticed that some models were better at writing Svelte 5 than others and that gave me idea of creating a Svelte-specific large language model benchmark. So for the past year or so, me together with a few other people have been testing basically every new popular LLM that was released with this. All of your GPTs, Geminis and Clouds were tested together with as many open source models as we had time for. As of today we've tested over 80 models for how well they churn out correct, best practices Svelte code. And this week something really exciting happened. We had a model score, a 100% perfect score. That means it wrote correct Svelte code on over 100 problems using a variety of different runes and other challenging tasks. In Sveltebench, each model is graded on a scale of 0 to 100. And when I started this experiment, a lot of even the most recent state-of-the-art models were struggling with this a lot For example GPT 4 Mini scored just 1 In fact it was the worst model we have ever tested This was especially worrying because GPT 4 Mini was and perhaps still is, the default model in GitHub Copilot. So if you install the most used developer tool for the most popular code editor, you would have an absolutely abysmal time if you try to do any sort of Svelte or SvelteKit coding. Other models were struggling too. The chat GPT version of 4.0 got 25.6%, Sony 3.7 got 56% and Gemini 2.5 Flash got about 70%. Writing code with a model that fails even relatively simple tasks half the time just isn't a great experience. But pretty quickly we saw the scores improve as new models with newer knowledge cutoffs were released. Newer cutoffs meant training on more recently written Svelte 5 code which means better result. And over time models kept pushing their scores higher and higher. GPT-5 was the first OpenAI model that got a decent score on the benchmark, scoring about 85%. Gemini continued pushing their already good performance with Gemini 3 Pro scoring 91%. We also saw the first OpenWeights models get decent scores. Now this is really important because as long as you only have closed proprietary models writing good svelte code you essentially locked into that provider and have to pay them whatever they want With open weights models you can run the models on your own hardware or on an open marketplace of providers like Parasail Runpod or Grok No not that one One of the first open weights models that had good scores was one of the DeepSeek chat models and more recently we've had several models with really good scores like Kimi K2, GLM 4.6 and recently Minimax M2. Now all of the above models are really huge. For example, the Kimi K2 model is a one trillion parameter model, and that means you wouldn't be running it on your laptop. But one really interesting space are small models that you could potentially run on your home computer and that still would provide a decent result. This is a bit of a complicated space because there are just so many small models and fine tunes of small models to test, but we've seen some interesting models like the ByteDance Seed OSS 36 billion parameter models scoring in the 70s on the Svelte Bench test. Let's get back to that 100% score I promised you. As the state-of-the-art models have been inching higher and higher to the 100% line, I think it was only a question of time before the first model got a perfect score, and the model that did this first is the brand new Opus 4.5 by Anthropic. This is honestly super impressive and I blown away that it only took roughly a year for models to improve to the point of writing Svelte 5 basically perfectly So what next for Svelte Bench now that it been conquered so to speak Recently the AI space has been moving into more agentic workflows and we had some good discussions both with the Svelte maintainers as well as some of the volunteers that have been helping out with Svelte Bench to discuss kind of where to go next from here. We think that the next step is to expand the benchmark with of course more tests but also to enable it to use the brand new Svelte MCP. In case you haven't seen it, Svelte recently launched an official MCP server. It's really easy to set up using the npxsv command, and it works with all the major coding tools such as Copilot and Cloud Code. The MCP has several different tools. For example, it gives your LLM access to updated documentation, and it also has a way to validate the code that the LLM has written and get suggestions for best practices. And that means, for example, no more colons in event handlers, finally. So if you haven't tried that out, give it a shot. Before ending this video, I want to give a huge shout out to everyone who has helped with Sveltebench over the past year. This has really been a community effort. If you want to contribute or just check out the benchmark results for yourself, I'll put a link in the video description. Let me know in the comments if you've been struggling with AI in Svelte 5 and whether this video gives you some hope for the future. Don't forget to like and subscribe if you found this useful and I'll see you in the next video.